# Ø¯Ù„ÙŠÙ„ ØªØ´ØºÙŠÙ„ LFM2.5-Thinking Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØµÙ†ÙŠÙ

## ğŸ“‹ Ù†Ø¸Ø±Ø© Ø³Ø±ÙŠØ¹Ø©

Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¢Ù† ÙŠØ¯Ø¹Ù… **Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† LLM**:

| Ø§Ù„Ù†Ù…ÙˆØ°Ø¬             | Ø§Ù„Ù…Ù†ÙØ°           | Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…                          |
| ------------------- | ---------------- | ---------------------------------- |
| **Qwen2.5-14B**     | `localhost:8000` | LLM Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (Ø³Ø±ÙŠØ¹ØŒ 1.2B params)    |
| **LFM2.5-Thinking** | `localhost:8001` | Ù†Ù…ÙˆØ°Ø¬ thinking (Ø¬Ø¯ÙŠØ¯ØŒ 1.2B params) |

---

## ğŸš€ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ´ØºÙŠÙ„

### 1ï¸âƒ£ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… LFM2.5 (Python)

Ø§ÙØªØ­ terminal Ø¬Ø¯ÙŠØ¯ ÙˆØ´ØºÙ‘Ù„:

```bash
cd "e:\yarab we elnby\New folder\Filmlane\python-llm-server"

# Install dependencies (first time only)
pip install -r requirements.txt

# Run the server
python server.py
```

**Ø£ÙˆÙ„ Ù…Ø±Ø© Ø³ÙŠØ­Ù…Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (~5-10 Ø¯Ù‚Ø§Ø¦Ù‚)** - Ø³ÙŠØ¸Ù‡Ø±:

```
ğŸš€ Starting LFM2.5 Classification Server...
INFO:     Started server process [1234]
INFO:     Waiting for application startup.
âœ… Model loaded successfully!
INFO:     Uvicorn running on http://127.0.0.1:8001
```

### 2ï¸âƒ£ ØªØ´ØºÙŠÙ„ Next.js (TypeScript)

Ø§ÙØªØ­ terminal Ø¢Ø®Ø±:

```bash
cd "e:\yarab we elnby\New folder\Filmlane"
npm run dev
```

---

## ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…

### Ø§Ø®ØªØ¨Ø§Ø± API Ù…Ø¨Ø§Ø´Ø±Ø©:

```bash
curl -X POST http://127.0.0.1:8001/classify \
  -H "Content-Type: application/json" \
  -d "{\"line\": \"ÙŠØ¯Ø®Ù„ Ø£Ø­Ù…Ø¯\"}"
```

### Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù† TypeScript:

```typescript
import { lfmClassifier } from '@/utils/classification/lfm-classifier';

const result = await lfmClassifier.classify(
  'ÙŠØ¯Ø®Ù„ Ø£Ø­Ù…Ø¯ Ø¥Ù„Ù‰ Ø§Ù„ØºØ±ÙØ©',
  'Ù…Ø´Ù‡Ø¯ 1 - Ù…Ù†Ø²Ù„',
  'scene-header',
);

console.log(result.type); // "action"
console.log(result.confidence); // 9.5
```

---

## ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©

| Ø§Ù„Ù…Ù„Ù                                        | Ø§Ù„ÙˆØµÙ                  |
| -------------------------------------------- | ---------------------- |
| `python-llm-server/server.py`                | FastAPI server Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ |
| `python-llm-server/requirements.txt`         | Ù…ÙƒØªØ¨Ø§Øª Python          |
| `src/app/api/lfm-proxy/route.ts`             | Next.js API proxy      |
| `src/utils/classification/lfm-classifier.ts` | TypeScript service     |

---

## ğŸ”§ Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ

### Ø§Ø³ØªØ®Ø¯Ø§Ù… LFM2.5 ÙÙŠ `paste-classifier.ts`:

```typescript
import { lfmClassifier } from './classification/lfm-classifier';

// Ø¨Ø¹Ø¯ decision engineØŒ Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Ù†Ø§ LLM:
if (decision.shouldUseLLM) {
  try {
    // Check if LFM is available
    const lfmAvailable = await lfmClassifier.healthCheck();

    if (lfmAvailable) {
      const result = await lfmClassifier.classify(
        line,
        ctx.previousLines.join(' | '),
        ctx.previousTypes.slice(-1)[0],
      );

      return {
        type: result.type,
        score: result.confidence * 10, // Scale to 0-10
        decision: { shouldUseLLM: false, reason: 'lfm_used' },
      };
    }
  } catch (e) {
    logger.warning('LFM unavailable, falling back to Qwen2.5');
  }

  // Fallback to Qwen2.5 (existing)
}
```

---

## âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª

### ØªÙØ¹ÙŠÙ„/ØªØ¹Ø·ÙŠÙ„ LFM2.5:

```typescript
import { lfmClassifier } from '@/utils/classification/lfm-classifier';

// Disable LFM2.5
lfmClassifier.setEnabled(false);

// Enable LFM2.5
lfmClassifier.setEnabled(true);
```

### ØªØºÙŠÙŠØ± timeout:

```typescript
lfmClassifier.setTimeout(60000); // 60 seconds
```

---

## âš¡ Ø§Ù„Ø£Ø¯Ø§Ø¡

| Ø§Ù„Ø¹Ù…Ù„                   | Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ù‚Ø¯Ø± |
| ----------------------- | ------------ |
| ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø£ÙˆÙ„ Ù…Ø±Ø©) | 5-10 Ø¯Ù‚Ø§Ø¦Ù‚   |
| ØªØµÙ†ÙŠÙ Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ (GPU)    | ~0.5 Ø«Ø§Ù†ÙŠØ©   |
| ØªØµÙ†ÙŠÙ Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ (CPU)    | ~5 Ø«ÙˆØ§Ù†      |
| ØªØµÙ†ÙŠÙ 10 Ø£Ø³Ø·Ø± (GPU)     | ~3 Ø«ÙˆØ§Ù†      |

---

## ğŸ› Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡

### Port 8001 Ù…Ø´ØºÙˆÙ„:

```python
# ØºÙŠØ± Ø§Ù„Ø¨ÙˆØ±Øª ÙÙŠ python-llm-server/server.py
uvicorn.run(app, host="127.0.0.1", port=8002)  # ØªØºÙŠÙŠØ± Ø¥Ù„Ù‰ 8002
```

### CUDA Out of Memory:

```python
# Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø³ÙŠØªØ­ÙˆÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¥Ù„Ù‰ CPU
# Ù„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¬Ø¨Ø§Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©:
# ÙÙŠ server.pyØŒ Ø§Ø³ØªØ¨Ø¯Ù„:
DEVICE = "cpu"
```

### ImportError: transformers:

```bash
pip install --upgrade transformers torch
```

---

## ğŸ¯ Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ø£Ø¯Ø§Ø¡

1. **Ø§Ø³ØªØ®Ø¯Ù… LFM2.5 Ù„Ù„Ø«Ù‚Ø© Ø§Ù„Ù…ØªÙˆØ³Ø·Ø© ÙÙ‚Ø·** - Ø£Ø¨Ø·Ø£ Ù…Ù† Qwen2.5 Ù„ÙƒÙ†Ù‡ "thinking model"
2. **Ø§Ø³ØªØ®Ø¯Ù… Qwen2.5 Ù„Ù„Ø³Ø±Ø¹Ø©** - Ø£Ø³Ø±Ø¹ Ù„ÙƒÙ† Ø¨Ø¯ÙˆÙ† thinking
3. **Batch requests** - Ø£Ø³Ø±Ø¹ Ù…Ù† Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª ÙØ±Ø¯ÙŠØ©
4. **GPU highly recommended** - 10x Ø£Ø³Ø±Ø¹ Ù…Ù† CPU
